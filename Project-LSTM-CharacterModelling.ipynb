{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, lets import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.stat_result(st_mode=33204, st_ino=6839431323, st_dev=41, st_nlink=1, st_uid=1000, st_gid=2000, st_size=1115393, st_atime=1567160289, st_mtime=1567160289, st_ctime=1567170750)\n",
      "2019-08-30 10:18:09.766575\n"
     ]
    }
   ],
   "source": [
    "#print(os.getcwd())\n",
    "#print(os.listdir())\n",
    "from datetime import datetime\n",
    "\n",
    "print(os.stat('input.txt'))\n",
    "\n",
    "mod_time = os.stat('input.txt').st_mtime\n",
    "\n",
    "print(datetime.fromtimestamp(mod_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Libraries\n",
    "<ul>\n",
    "    <li><b>os</b>: is an module that allows us to interact with the operating systerm, in particular we will use it to set the path in which we will be storing our input file, tensor file and vocab file</li>\n",
    "    <li><b>time</b>: is a library that allows us to access the clock time of our machine, we will use it to measure the performance of training our model with a CPU, versus training our model with a GPU</li>\n",
    "    <li><b>cPickle</b>: is a library for serializing and deserializing python objects, we will use the <b>dump()</b> method in cPickle to serialize our objects when saving them, and <b>load()</b> method in cPickle to deserialize our objects when loading.</li>\n",
    "    <li><b>codec</b>: is a library that deals with character encoding, we will use the <b>open()</b> method as it is recommended when opening encoded text files.</li>\n",
    "    <li><b>collections</b>: is a library that implements high performance container types, we will use the <b>Counter</b> object to get a collection of frequencies for our characters</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Downloading the input data</h3>\n",
    "Lets download the input file, and take a look at some parts of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-30 13:17:09 URL:https://public.boxcloud.com/d/1/b1!YgTw2MrsGCRS-AakUwUc8OW7iX2DBvKnaeI9WlBvPoP1wqhJom2CisZ4KmDCezCPmkIpJE6sPgy772dldTVDLf2HTTaJPME_WnNZ_oxKRL69UZ1cxpnzwaIpzQ3Ztb_1p0qFOTTQwF482jp2f3v0sCGnpvYm2AfhNwrhiGzhj2fxzfsR5V-qJTfnmD065fP_Mo57LCnNz4WMGTgXvZOabcWYTTOAd-V4EvNQfh3ozKDptxr7_l5aWtFmFqIErcmZJ0uokCB6XdyIQqpHrRnr_kxneRXEtILYYJs4zOb_2_3ZCvqIP1Kk9qD0kD1N1j63Aw5bY1kwkktWQxaLLysk_v0jXqrgAa6tbyOjJALLWv_dNG7uanJwRzKIYY19IHEJKJCKBKIFQ4PMO-GIMZypz-dgVJWBt3DlFb4tAKthRsf11wzOh-q0pHFKgh_aao7rteNJvJFB4izz9MGV-INDw74ePPgbVUGVybV--tL2NdTGHIZJ_CURyXztDj-U8v0LopygpJnKGSOkkWO_Lkb8dVdbEAwf4HSKI-rNlSfVxz5VSkCbYsL4OFE9njKAQ3GPVf4AHXTG3rBzIi1d27RwtEmjH0ge4jS2HsiNMXINBv5O2XgsMcwnRgsugui8E2y0d1rfPTfoeQZWvgMtWmPga-LfMTpzl-P1su_8tHIO2qeole4I2iYGRJ-GCeNRt6qMKE2VZvxn77JnlpVAx-5JAdEGsTixyHfi_LGb5imzZxafbDJtwfTfhN5qKsNI1c-e1ce-2aFp0wDEoyqRJKQOTQkVx5PJdIX87Vzxnw_MgiUjOCVWiOsmjXsgavETRGIYSzd95JaRDxH8lEnJtwE5ZtuO6kyRtPZUcMKLR0EJuW2Dk4eM5ONZwjfqyWSliyjvMVsRk4ShYIG2a9lfp1wTCMjF60HImzN6aS9aaDcl9Ht2gyL2k2wnqOAxzYw6hNkD6-yIcLmKM-6V2-Y8LI1dP4VfjvQ0-tGaN7qK2TbPOYhSwiZ40CDYKrcHeN969YpdHfFE8P-zMakUASe_rr4dlUCGLxLUXObdvlp90CbMRpR4DZG32GLUj5HExwfNQH6wkPBiuLinCuQPkGc2skqohh4IFBDRWE7tfIpCI_Q1hCWEioFJvOz07UaUCyOfEmllaTJTbYtzhrAKKsvJjzT1CnkXyBGRWfbGjUYfken-Y-R5RtN8RQmnbm92gdrMTz_jewINUe9nLQhVhHX8C4VIWS9gevtTR3UEnaNhdaKoi3m7kHtODRzXvOZhIvj76aRlj2k4a508Ri26xHdOtBlEt0G5pqz7vJWGO6PLNn6HfGwwrQmMN9F8zmNQHq8EeOc1VOSzAVFaYEhg22iZo3Y1eo2t_YLaw0E2k62bdY4Om9e73kna08zv/download [1115393/1115393] -> \"input.txt\" [1]\n",
      "-------------Sample text---------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print(\"-------------Sample text---------------\")\n",
    "    print (read_data[0:500])\n",
    "    print(\"---------------------------------------\")\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Data loader</h3>\n",
    "One need to read the input file and convert each character to numerical values. The following cell is a class that helps to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Parameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>We have to convert the characters in the string to numbers. Also we need to represent each sequence of characters as a vector in each batch.</p>\n",
    "So, let's set some parameters that we need those now for reading the dataset, and later to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 128  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 50 # one should change it to 50 if relatively good results are desired\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>Now, we can read the data at batches using the <b>TextLoader</b> class. It will convert the characters to numbers, and represent each sequence as a vector in batches:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "vocabulary size: 65\n",
      "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [39  5  3 ...  0 20  9]\n",
      " [ 0  5  9 ... 19  4 13]\n",
      " ...\n",
      " [ 3 18 18 ...  1  0 23]\n",
      " [ 7  1 23 ... 18  3  7]\n",
      " [47 26 24 ...  0  8  3]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print (\"vocabulary size:\" ,data_loader.vocab_size)\n",
    "print (\"Characters:\" ,data_loader.chars)\n",
    "print (\"vocab number of 'F':\",data_loader.vocab['F'])\n",
    "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>Notice:</b> In the following cells, we just go through the process of defining each element of the LSTM, and explore the inputs, outputs of each layer. Then, we put all together and run the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>1- Input and Output</h3>\n",
    "In the next cell we just take a look at a sample batch to underestand the data better. Each batch includes the input, <b>x</b>, and the character that we want to predict, <b>y</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [39,  5,  3, ...,  0, 20,  9],\n",
       "       [ 0,  5,  9, ..., 19,  4, 13],\n",
       "       ...,\n",
       "       [ 3, 18, 18, ...,  1,  0, 23],\n",
       "       [ 7,  1, 23, ..., 18,  3,  7],\n",
       "       [47, 26, 24, ...,  0,  8,  3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =128, seq_length=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, y is the next character for each character in x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 5,  3,  0, ..., 20,  9, 27],\n",
       "       [ 5,  9, 14, ...,  4, 13, 20],\n",
       "       ...,\n",
       "       [18, 18,  9, ...,  0, 23, 11],\n",
       "       [ 1, 23,  3, ...,  3,  7,  0],\n",
       "       [26, 24, 10, ...,  8,  3,  2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>3- Defining stacked RNN Cell</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>BasicRNNCell</b> is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to define a LSTM cell\n",
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>state</b> variable keeps output and new_state of the LSTM, so it is double in size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets define the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(128, 50) dtype=int32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 128x50\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "and target data, what we want to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(128, 50) dtype=int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 128x50\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "<b>BasicRNNCell.zero_state(batch_size, dtype)</b> Return zero-filled state tensor(s). In this function, batch_size\n",
    "representing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState/zeros:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState_1/zeros:0' shape=(128, 128) dtype=float32>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) \n",
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets check the value of the input_data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [39,  5,  3, ...,  0, 20,  9],\n",
       "       [ 0,  5,  9, ..., 19,  4, 13],\n",
       "       ...,\n",
       "       [ 3, 18, 18, ...,  1,  0, 23],\n",
       "       [ 7,  1, 23, ..., 18,  3,  7],\n",
       "       [47, 26, 24, ...,  0,  8,  3]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session(config=config)\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>4- Embedding</h3>\n",
    "<p>In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix.</p>\n",
    "\n",
    "<p><b>Notice:</b> The function <code>tf.get_variable()</code> is used to share a variable and to initialize it in one place. <code>tf.get_variable()</code> is used to get or create a variable instead of a direct call to <code>tf.Variable</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets take a look at the <b>embedding</b>, <b>em</b>, and <b>inputs</b> variables:\n",
    "\n",
    "Embedding variable is initialized with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04869516, -0.17132466,  0.08786125, ...,  0.09496872,\n",
       "        -0.03553957, -0.06146944],\n",
       "       [ 0.06587443,  0.17524244, -0.13502207, ..., -0.11538697,\n",
       "        -0.12743324,  0.17174132],\n",
       "       [-0.1613645 , -0.02943245, -0.1473983 , ...,  0.14757879,\n",
       "        -0.12204084, -0.01753469],\n",
       "       ...,\n",
       "       [-0.17368922, -0.164241  ,  0.17366342, ...,  0.04329385,\n",
       "         0.03827263,  0.02882816],\n",
       "       [-0.11447039,  0.09980626, -0.00347814, ...,  0.05745204,\n",
       "        -0.02564628,  0.05669166],\n",
       "       [-0.02557629, -0.04158674, -0.14265817, ...,  0.04142044,\n",
       "        -0.03934956,  0.1581928 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "#print embedding.shape\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00936982,  0.01546817, -0.07208177, ...,  0.07416238,\n",
       "        -0.06070516, -0.07181732],\n",
       "       [-0.13115823,  0.03976522, -0.15710235, ..., -0.1414805 ,\n",
       "         0.11360596, -0.01539102],\n",
       "       [-0.10667104, -0.06048652, -0.05224667, ...,  0.09774043,\n",
       "         0.12739457,  0.16572614],\n",
       "       ...,\n",
       "       [ 0.06587443,  0.17524244, -0.13502207, ..., -0.11538697,\n",
       "        -0.12743324,  0.17174132],\n",
       "       [ 0.0109137 ,  0.13604014,  0.12487151, ...,  0.05502316,\n",
       "         0.141964  ,  0.16444208],\n",
       "       [-0.10667104, -0.06048652, -0.05224667, ...,  0.09774043,\n",
       "         0.12739457,  0.16572614]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print (emp.shape)\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>Let's consider each sequence as a sentence of length 50 characters, then, the first item in <b>inputs</b> is a [60x128] vector which represents the first characters of 60 sentences.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(128, 128) dtype=float32>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>5- Feeding a batch of 50 sequence to a RNN:</h3>\n",
    "\n",
    "The feeding process for inputs is as following:\n",
    "<ul>\n",
    "    <li>Step 1: first character of each of the 50 sentences (in a batch) is entered in parallel.</li>  \n",
    "    <li>Step 2: second character of each of the 50 sentences is input in parallel.</li> \n",
    "    <li>Step n: nth character of each of the 50 sentences is input in parallel.</li>  \n",
    "</ul>\n",
    "<p>The parallelism is only for efficiency. Each character in a batch is handled in parallel, but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00936982,  0.01546817, -0.07208177, ...,  0.07416238,\n",
       "        -0.06070516, -0.07181732],\n",
       "       [-0.00204353,  0.11974736, -0.10595191, ...,  0.01319008,\n",
       "        -0.16712162,  0.08638029],\n",
       "       [ 0.04869516, -0.17132466,  0.08786125, ...,  0.09496872,\n",
       "        -0.03553957, -0.06146944],\n",
       "       ...,\n",
       "       [ 0.12801419,  0.09182973, -0.12704049, ...,  0.00563568,\n",
       "         0.16816036,  0.11782263],\n",
       "       [-0.10667104, -0.06048652, -0.05224667, ...,  0.09774043,\n",
       "         0.12739457,  0.16572614],\n",
       "       [-0.17107163,  0.0318604 ,  0.01951259, ..., -0.0433607 ,\n",
       "        -0.1358027 , -0.1721824 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Feeding the RNN with one batch, we can check the new output and new state of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(128, 128) dtype=float32>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(128, 128) dtype=float32>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's check the output of network after feeding it with first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02469741, -0.01459357, -0.06040001, ..., -0.02076126,\n",
       "         0.08457284,  0.04603299],\n",
       "       [ 0.00202414,  0.03962038, -0.04982498, ..., -0.03215291,\n",
       "         0.05855769,  0.15409017],\n",
       "       [-0.12518369, -0.08340981, -0.06193903, ..., -0.03262179,\n",
       "        -0.01758838,  0.06998984],\n",
       "       ...,\n",
       "       [ 0.06690809, -0.09092592, -0.06852615, ..., -0.03115801,\n",
       "         0.01808828,  0.05549588],\n",
       "       [ 0.04016316,  0.00270185, -0.0698784 , ...,  0.01396697,\n",
       "         0.03011952,  0.12149755],\n",
       "       [ 0.06099113, -0.0573225 , -0.00505273, ...,  0.0149193 ,\n",
       "        -0.00032446, -0.06432496]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>As it was explained, <b>outputs</b> variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The <b>softmax_w</b> shape is [rnn_size, vocab_size], which is [128x65] in our case. Therefore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the <b>softmax(output * softmax_w + softmax_b)</b> for this purpose. The shape of the matrixis would be:</p>\n",
    "\n",
    "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can do it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(6400, 128) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(6400, 65) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(6400, 65) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here is the probablity of the next chracter in all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01870732, 0.01603062, 0.01389957, ..., 0.01622879, 0.01580931,\n",
       "        0.01530458],\n",
       "       [0.01333357, 0.01883012, 0.01454788, ..., 0.01515247, 0.02155875,\n",
       "        0.01603586],\n",
       "       [0.01759003, 0.01856367, 0.01459633, ..., 0.01566401, 0.01704178,\n",
       "        0.01834561],\n",
       "       ...,\n",
       "       [0.01429179, 0.01391493, 0.01814258, ..., 0.01736828, 0.02144393,\n",
       "        0.0133232 ],\n",
       "       [0.02329761, 0.01759953, 0.01408618, ..., 0.01122731, 0.01238288,\n",
       "        0.01765132],\n",
       "       [0.01244864, 0.01383867, 0.01656101, ..., 0.02109144, 0.01805137,\n",
       "        0.01753136]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we are in the position to calculate the cost of training with <b>loss function</b>, and keep feeding the network to learn it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Okay, by now, you should understand enough about each component of a LSTM network to be able to train it, and predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2>All together</h2>\n",
    "Now, let's put all of parts together in a class, and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False, device='/cpu:0'):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 128 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        with tf.device(device):\n",
    "            # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "            basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "            # model.cell.state_size is (128, 128)\n",
    "            self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "            self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "            self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "            # Initial state of the LSTM memory.\n",
    "            # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "            self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "            with tf.variable_scope('rnnlm_class1'):\n",
    "                softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "                softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs] \n",
    "\n",
    "            # The value of state is updated after processing each batch of chars.\n",
    "            outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "            output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "            self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "            loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                    [tf.reshape(self.targets, [-1])],\n",
    "                    [tf.ones([batch_size * seq_length])],\n",
    "                    vocab_size)\n",
    "            self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "            self.final_state = last_state\n",
    "            self.lr = tf.Variable(0.0, trainable=False)\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"cpu_vs_gpu\"></a>\n",
    "<h2>Train the model using CPU and GPU</h2>\n",
    "We can train our model through feeding batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_1\"></a>\n",
    "<h2>Code to run it on CPU</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/8700 (epoch 0), train_loss = 2.082, time/batch = 0.111\n",
      "347/8700 (epoch 1), train_loss = 1.874, time/batch = 0.113\n",
      "521/8700 (epoch 2), train_loss = 1.762, time/batch = 0.080\n",
      "695/8700 (epoch 3), train_loss = 1.691, time/batch = 0.115\n",
      "869/8700 (epoch 4), train_loss = 1.642, time/batch = 0.116\n",
      "1043/8700 (epoch 5), train_loss = 1.606, time/batch = 0.111\n",
      "1217/8700 (epoch 6), train_loss = 1.580, time/batch = 0.115\n",
      "1391/8700 (epoch 7), train_loss = 1.559, time/batch = 0.115\n",
      "1565/8700 (epoch 8), train_loss = 1.538, time/batch = 0.083\n",
      "1739/8700 (epoch 9), train_loss = 1.522, time/batch = 0.114\n",
      "1913/8700 (epoch 10), train_loss = 1.509, time/batch = 0.113\n",
      "2087/8700 (epoch 11), train_loss = 1.498, time/batch = 0.115\n",
      "2261/8700 (epoch 12), train_loss = 1.488, time/batch = 0.120\n",
      "2435/8700 (epoch 13), train_loss = 1.480, time/batch = 0.113\n",
      "2609/8700 (epoch 14), train_loss = 1.472, time/batch = 0.109\n",
      "2783/8700 (epoch 15), train_loss = 1.466, time/batch = 0.116\n",
      "2957/8700 (epoch 16), train_loss = 1.460, time/batch = 0.112\n",
      "3131/8700 (epoch 17), train_loss = 1.455, time/batch = 0.076\n",
      "3305/8700 (epoch 18), train_loss = 1.450, time/batch = 0.115\n",
      "3479/8700 (epoch 19), train_loss = 1.446, time/batch = 0.113\n",
      "3653/8700 (epoch 20), train_loss = 1.442, time/batch = 0.081\n",
      "3827/8700 (epoch 21), train_loss = 1.439, time/batch = 0.115\n",
      "4001/8700 (epoch 22), train_loss = 1.436, time/batch = 0.117\n",
      "4175/8700 (epoch 23), train_loss = 1.433, time/batch = 0.112\n",
      "4349/8700 (epoch 24), train_loss = 1.430, time/batch = 0.117\n",
      "4523/8700 (epoch 25), train_loss = 1.428, time/batch = 0.083\n",
      "4697/8700 (epoch 26), train_loss = 1.425, time/batch = 0.117\n",
      "4871/8700 (epoch 27), train_loss = 1.423, time/batch = 0.116\n",
      "5045/8700 (epoch 28), train_loss = 1.421, time/batch = 0.116\n",
      "5219/8700 (epoch 29), train_loss = 1.418, time/batch = 0.115\n",
      "5393/8700 (epoch 30), train_loss = 1.417, time/batch = 0.118\n",
      "5567/8700 (epoch 31), train_loss = 1.415, time/batch = 0.119\n",
      "5741/8700 (epoch 32), train_loss = 1.413, time/batch = 0.115\n",
      "5915/8700 (epoch 33), train_loss = 1.411, time/batch = 0.078\n",
      "6089/8700 (epoch 34), train_loss = 1.410, time/batch = 0.113\n",
      "6263/8700 (epoch 35), train_loss = 1.408, time/batch = 0.081\n",
      "6437/8700 (epoch 36), train_loss = 1.407, time/batch = 0.115\n",
      "6611/8700 (epoch 37), train_loss = 1.405, time/batch = 0.113\n",
      "6785/8700 (epoch 38), train_loss = 1.404, time/batch = 0.119\n",
      "6959/8700 (epoch 39), train_loss = 1.403, time/batch = 0.117\n",
      "7133/8700 (epoch 40), train_loss = 1.401, time/batch = 0.117\n",
      "7307/8700 (epoch 41), train_loss = 1.400, time/batch = 0.110\n",
      "7481/8700 (epoch 42), train_loss = 1.399, time/batch = 0.117\n",
      "7655/8700 (epoch 43), train_loss = 1.398, time/batch = 0.114\n",
      "7829/8700 (epoch 44), train_loss = 1.397, time/batch = 0.111\n",
      "8003/8700 (epoch 45), train_loss = 1.396, time/batch = 0.116\n",
      "8177/8700 (epoch 46), train_loss = 1.395, time/batch = 0.117\n",
      "8351/8700 (epoch 47), train_loss = 1.394, time/batch = 0.121\n",
      "8525/8700 (epoch 48), train_loss = 1.393, time/batch = 0.112\n",
      "8699/8700 (epoch 49), train_loss = 1.393, time/batch = 0.116\n"
     ]
    }
   ],
   "source": [
    "avg_batch_running_duration_CPU=[]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn_CPU\"):\n",
    "    model = LSTMModel(device='/cpu:0')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(num_epochs): # num_epochs is 20 for test, but should be higher\n",
    "            sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "            ##  code below to reset the batch pointer in data_loader. \n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state) # model initialization\n",
    "            batch_running_duration_CPU = []\n",
    "            for b in range(data_loader.num_batches): #for each batch\n",
    "                start = time.time()\n",
    "                ## code to define your x and y. \n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "\n",
    "                ## code to train the model\n",
    "                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op],feed)\n",
    "\n",
    "                end = time.time()\n",
    "\n",
    "                ## code to store the duration of runing each batch in a list (end - start)\n",
    "                batch_running_duration_CPU.append(end - start)\n",
    "        \n",
    "            print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                    .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "            avg_batch_running_duration_CPU.append(sum(batch_running_duration_CPU) / float(len(batch_running_duration_CPU)))\n",
    "\n",
    "            # To see the sample of prediction\n",
    "#            with tf.variable_scope(\"rnn_CPU\", reuse=True):\n",
    "#                  sample_model = LSTMModel(sample=True)\n",
    "#                  print ('----------------------------------')\n",
    "#                  print ('SAMPLE GENERATED TEXT:')\n",
    "#                  print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))\n",
    "#                  print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_2\"></a>\n",
    "<h2>Code to run it on GPU</h2>\n",
    "Now, the same network with GPU, and the time/batch for running each batch is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/8700 (epoch 0), train_loss = 2.060, time/batch = 0.019\n",
      "347/8700 (epoch 1), train_loss = 1.851, time/batch = 0.020\n",
      "521/8700 (epoch 2), train_loss = 1.741, time/batch = 0.020\n",
      "695/8700 (epoch 3), train_loss = 1.674, time/batch = 0.019\n",
      "869/8700 (epoch 4), train_loss = 1.630, time/batch = 0.019\n",
      "1043/8700 (epoch 5), train_loss = 1.598, time/batch = 0.021\n",
      "1217/8700 (epoch 6), train_loss = 1.572, time/batch = 0.019\n",
      "1391/8700 (epoch 7), train_loss = 1.552, time/batch = 0.022\n",
      "1565/8700 (epoch 8), train_loss = 1.536, time/batch = 0.018\n",
      "1739/8700 (epoch 9), train_loss = 1.522, time/batch = 0.021\n",
      "1913/8700 (epoch 10), train_loss = 1.511, time/batch = 0.018\n",
      "2087/8700 (epoch 11), train_loss = 1.502, time/batch = 0.019\n",
      "2261/8700 (epoch 12), train_loss = 1.494, time/batch = 0.021\n",
      "2435/8700 (epoch 13), train_loss = 1.487, time/batch = 0.020\n",
      "2609/8700 (epoch 14), train_loss = 1.480, time/batch = 0.021\n",
      "2783/8700 (epoch 15), train_loss = 1.474, time/batch = 0.020\n",
      "2957/8700 (epoch 16), train_loss = 1.469, time/batch = 0.019\n",
      "3131/8700 (epoch 17), train_loss = 1.464, time/batch = 0.019\n",
      "3305/8700 (epoch 18), train_loss = 1.460, time/batch = 0.018\n",
      "3479/8700 (epoch 19), train_loss = 1.455, time/batch = 0.019\n",
      "3653/8700 (epoch 20), train_loss = 1.451, time/batch = 0.018\n",
      "3827/8700 (epoch 21), train_loss = 1.447, time/batch = 0.018\n",
      "4001/8700 (epoch 22), train_loss = 1.444, time/batch = 0.020\n",
      "4175/8700 (epoch 23), train_loss = 1.441, time/batch = 0.020\n",
      "4349/8700 (epoch 24), train_loss = 1.438, time/batch = 0.021\n",
      "4523/8700 (epoch 25), train_loss = 1.435, time/batch = 0.018\n",
      "4697/8700 (epoch 26), train_loss = 1.432, time/batch = 0.018\n",
      "4871/8700 (epoch 27), train_loss = 1.430, time/batch = 0.019\n",
      "5045/8700 (epoch 28), train_loss = 1.427, time/batch = 0.019\n",
      "5219/8700 (epoch 29), train_loss = 1.425, time/batch = 0.019\n",
      "5393/8700 (epoch 30), train_loss = 1.423, time/batch = 0.019\n",
      "5567/8700 (epoch 31), train_loss = 1.420, time/batch = 0.019\n",
      "5741/8700 (epoch 32), train_loss = 1.419, time/batch = 0.019\n",
      "5915/8700 (epoch 33), train_loss = 1.417, time/batch = 0.018\n",
      "6089/8700 (epoch 34), train_loss = 1.415, time/batch = 0.019\n",
      "6263/8700 (epoch 35), train_loss = 1.413, time/batch = 0.020\n",
      "6437/8700 (epoch 36), train_loss = 1.412, time/batch = 0.019\n",
      "6611/8700 (epoch 37), train_loss = 1.410, time/batch = 0.019\n",
      "6785/8700 (epoch 38), train_loss = 1.409, time/batch = 0.020\n",
      "6959/8700 (epoch 39), train_loss = 1.407, time/batch = 0.020\n",
      "7133/8700 (epoch 40), train_loss = 1.406, time/batch = 0.018\n",
      "7307/8700 (epoch 41), train_loss = 1.405, time/batch = 0.019\n",
      "7481/8700 (epoch 42), train_loss = 1.403, time/batch = 0.021\n",
      "7655/8700 (epoch 43), train_loss = 1.402, time/batch = 0.018\n",
      "7829/8700 (epoch 44), train_loss = 1.401, time/batch = 0.020\n",
      "8003/8700 (epoch 45), train_loss = 1.400, time/batch = 0.021\n",
      "8177/8700 (epoch 46), train_loss = 1.399, time/batch = 0.019\n",
      "8351/8700 (epoch 47), train_loss = 1.399, time/batch = 0.018\n",
      "8525/8700 (epoch 48), train_loss = 1.398, time/batch = 0.019\n",
      "8699/8700 (epoch 49), train_loss = 1.397, time/batch = 0.019\n"
     ]
    }
   ],
   "source": [
    "### Create the same network using GPU\n",
    "\n",
    "\n",
    "avg_batch_running_duration_GPU=[]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn_GPU\"):\n",
    "    model = LSTMModel(device='/gpu:0')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(num_epochs): # num_epochs is 20 for test, but should be higher\n",
    "            sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "            ##  code below to reset the batch pointer in data_loader. \n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state) # model initialization\n",
    "            batch_running_duration_GPU = []\n",
    "            for b in range(data_loader.num_batches): #for each batch\n",
    "                start = time.time()\n",
    "                ## code to define your x and y. \n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "\n",
    "                feed = {model.input_data: x, model.targets: y, model.initial_state:state}            \n",
    "\n",
    "                ## code to train the model\n",
    "                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op],feed)\n",
    "\n",
    "\n",
    "                end = time.time()\n",
    "\n",
    "                ## code to store the duration of runing each batch in a list (end - start)\n",
    "                batch_running_duration_GPU.append(end - start)\n",
    "\n",
    "\n",
    "            print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                    .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "            avg_batch_running_duration_GPU.append(sum(batch_running_duration_GPU) / float(len(batch_running_duration_GPU)))\n",
    "\n",
    "            # To see the sample of prediction\n",
    "#            with tf.variable_scope(\"rnn_GPU\", reuse=True):\n",
    "#                  sample_model = LSTMModel(sample=True)\n",
    "#                  print ('----------------------------------')\n",
    "#                  print ('SAMPLE GENERATED TEXT:')\n",
    "#                  print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=200, prime='The ', sampling_type=1))\n",
    "#                  print ('----------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_3\"></a>\n",
    "<h2>Compare the results</h2>\n",
    "Finally, using a graph, the speed of training (batch/time) for the model running on GPU and CPU is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets plot the performance of training the model on a CPU versus on a GPU.\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(1,len(avg_batch_running_duration_GPU)+1)), avg_batch_running_duration_GPU, label='GPU')\n",
    "plt.plot(list(range(1,len(avg_batch_running_duration_CPU)+1)), avg_batch_running_duration_CPU, label='CPU')\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('Number of epoch ')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
